\documentclass[11pt,notitlepage,a4paper]{article}
%\usepackage[a4paper,hmargin=1.3in,vmargin=1.3in]{geometry}
\usepackage[
  bookmarks=true,       % show bookmarks bar?
  unicode=true,         % non-Latin characters in Acrobat’s bookmarks
  pdftoolbar=false,     % show Acrobat’s toolbar?
  pdfmenubar=false,     % show Acrobat’s menu?
  pdffitwindow=false,   % window fit to page when opened
  pdfstartview={FitH},  % fits the width of the page to the window
  pdftitle={BLAKE3},    % title
  pdfauthor={TODO},     % author
  pdfsubject={BLAKE3},  % subject of the document
  pdfnewwindow=true,    % links in new window
  colorlinks=true,      % false: boxed links; true: colored links
  linkcolor=orangered,  % color of internal links
  citecolor=orangered,   % color of links to bibliography
  filecolor=magenta,    % color of file links
  urlcolor=orangered,   % color of external links
]{hyperref}
\usepackage{url}
%\usepackage{fullpage}
\usepackage{a4wide}
\usepackage{amsmath,amssymb,cite,dsfont}

\usepackage{verbatim,footmisc}
\usepackage{array}

%\usepackage{euler}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
%\fontfamily{phv}\selectfont

\renewcommand{\familydefault}{\sfdefault}
\usepackage[scaled=1]{helvet}
\usepackage[helvet]{sfmath}
\everymath={\sf}

\usepackage[T1]{fontenc} % improves underscore rendering

\usepackage{textcomp}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{datetime}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage[english]{babel}
\usepackage[autostyle, english=american]{csquotes}

\usepackage{xspace}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{tikz}
\usepackage[shortcuts]{extdash} % unbreakable dashes, e.g. SHA\=/2
\usepackage[outputdir=build]{minted} % syntax highlighting using Pygments
\usepackage[title]{appendix} % add "Appendix" to each appendix section title
\usepackage[font=small,labelfont=bf]{caption}
\usetikzlibrary
{
  calc,
  positioning,
  shapes,
  shapes.geometric,
  arrows
}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

\definecolor{darkred}{RGB}{170,0,0}
\definecolor{darkblue}{RGB}{0,0,170}
\definecolor{darkgreen}{RGB}{0,170,0}
\definecolor{orangered}{RGB}{255,69,0}
\definecolor{bluecompl}{RGB}{0,186,255}

\newdateformat{mydate}{\THEYEAR.\twodigit\THEMONTH.\twodigit\THEDAY}

\newcommand{\GG}{\mathsf{G}}
\newcommand{\IV}{\text{IV}}
\newcommand{\cpress}{\text{\textbf{compress}}}
\newcommand{\lto}{\leftarrow}
\newcommand{\BB}{\mathsf{B2}}

\newcommand{\flag}[1]{\texttt{\detokenize{#1}}\xspace}

\newcommand{\alert}[1]{\textcolor{red}{#1}}

\newcommand{\mytitle}{BLAKE3}

\title{\mytitle}


\begin{document}

\fontfamily{phv}\selectfont
\pagestyle{plain}

\let\thefootnote\relax\footnote{Version \texttt{\pdfdate}.}
\addtocounter{footnote}{-1}\let\thefootnote\svthefootnote

\begin{center}
{\Huge \bf \mytitle}

\medskip

{\Large \bf  one function, fast everywhere}

\medskip

Jack O'Connor (\texttt{@oconnor663}) \\
Jean-Philippe Aumasson (\texttt{@veorq}) \\
Samuel Neves (\texttt{@sevenps}) \\
Zooko Wilcox-O'Hearn (\texttt{@zooko}) \\

\medskip

{\large \url{https://blake3.io}}

\end{center}


\medskip

\begin{center}
  \begin{minipage}{0.92\linewidth}

      We present BLAKE3, an evolution of the hash BLAKE2 that is faster,
      simpler to use, and better suited to applications' needs.
      BLAKE3 supports an unbounded degree of parallelism, using a tree
      structure that scales up to any number of SIMD lanes and CPU
      cores.  
      On Intel Kaby Lake, peak single-threaded throughput is $3\times$
      that of BLAKE2b, $4\times$ that of SHA\=/2, and $8\times$ that of
      SHA\=/3, and it scales further using multiple threads. 
      BLAKE3 is also efficient on smaller architectures and
      microcontrollers: throughput on the 32-bit ARM1176 core is
      $1.3\times$ that of BLAKE2s or SHA\=/256, and $3\times$ that of
      BLAKE2b, SHA\=/512, or SHA\=/3. 
      Unlike BLAKE2 and SHA\=/2, which have incompatible variants better
      suited for different platforms, BLAKE3 is a single hash function,
      designed to be consistently fast across a wide variety of software
      platforms and use cases.

   \end{minipage}
\end{center}

\newpage

\tableofcontents

\newpage

\section{Introduction}\label{sec:intro}

Since its announcement in 2012, BLAKE2~\cite{DBLP:conf/acns/AumassonNWW13} has 
seen widespread adoption, in large
part because of its superior performance in software. 
BLAKE2b and BLAKE2s are included in OpenSSL and in the Python and Go
standard libraries. 
BLAKE2b is also included as the \texttt{b2sum} utility in GNU Coreutils,
as the \texttt{generichash} API in Libsodium, and as the underlying hash function for
Argon2~\cite{DBLP:conf/eurosp/BiryukovDK16}, the winner of the Password Hashing Competition in 2015.

The biggest changes from BLAKE2 to BLAKE3 are:

\begin{itemize}
    \item An \textbf{internal tree structure}.
    \item A compression function with \textbf{fewer rounds}.
    \item A \textbf{single hash function}, with no variants or flavors.
    \item In lieu of a parameter block, an API supporting \textbf{three
        domain-separated modes}: \flag{hash(input)},
        \flag{keyed_hash(key, input)}, and \flag{derive_key(key,
        context)}. These modes differ only in the internal flag bits they set.
    \item The space formerly occupied by the parameter block is now used for
        the optional 256-bit key, so \textbf{keying is zero-cost}.
    \item BLAKE3 has built-in support for \textbf{extendable output}. Like
        BLAKE2X, but unlike SHA\=/3 or HKDF, extended output is parallelizable
        and seekable.
\end{itemize}

BLAKE3 thus eliminates the main drawback of BLAKE2, namely its number of
incompatible variants;
the original BLAKE2 paper described 64-bit BLAKE2b, 32-bit BLAKE2s, the parallel
variants BLAKE2bp and BLAKE2sp, a framework for tree modes, and the
BLAKE2X paper later added extendable output modes. 
None of these are compatible with each other, and choosing the right one
for an application means understanding both the tradeoffs between them
and also the state of language and library support.  
BLAKE2b, the most widely supported, is not the fastest on most platforms.
BLAKE2bp and BLAKE2sp, with much higher peak throughput, are sparsely
supported and rarely adopted.
BLAKE3 instead is a single hash function, with no variants, designed to
support all the use cases of BLAKE2, as well as new use cases like
verified streaming (see \S\ref{sec:verifiedstreaming}).

BLAKE3 is also dramatically faster.
For example, on an Intel Kaby Lake processor peak throughput on a single core is triple
that of BLAKE2b, and BLAKE3 can scale further to any number of cores. On an
ARM1176, throughput is $1.3\times$ that of BLAKE2s and again triple that of
BLAKE2b.
To achieve such speed, BLAKE3 splits its input into 1~KiB chunks and
arranges those chunks as the leaves of a binary tree. 
This tree structure means that there is no limit to the parallelism that
BLAKE3 can exploit, given enough input~\cite{DBLP:journals/tosc/AtighehchiB17,DBLP:journals/tc/AtighehchiR17}. 
The direct benefit of this parallelism is very high throughput on
platforms with SIMD support, including all modern x86 processors.
Another benefit of hashing chunks in parallel is that the implementation
can use SIMD vectors of any width, regardless of the word size of the
compression function. 
That leaves us free to use a compression function that is efficient on
smaller architectures, without sacrificing peak throughput on x86\=/64.

The BLAKE3 compression function is closely based on that of BLAKE2s.
BLAKE3 has the same 128-bit security level and 256-bit default output
size. The round function is identical, along with the IV constants \alert{and
the message schedule. Thus, cryptanalysis of BLAKE2 applies directly to
BLAKE3.} Based on that analysis and with the benefit of hindsight, we
believe that BLAKE2 is overly conservative in its number of rounds, and
BLAKE3 reduces the number of rounds from 10 to 7 (see~\cite{TMC} for
detailed rationale).
BLAKE3 also changes the setup and finalization steps of the compression
function to support the internal tree structure, more efficient keying,
and extendable output.

\section{Specification}\label{sec:specification}

\subsection{Tree Structure}\label{sec:tree}

BLAKE3 splits its input into chunks of up to 1024 bytes and arranges those
chunks as the leaves of a binary tree. The last chunk may be shorter, but not
empty, unless the entire input is empty. If there is only one chunk, that chunk
is the root node and only node of the tree. Otherwise, the chunks are assembled
with parent nodes, each parent node having exactly two children. The
structure of the tree is determined by two rules:
\begin{enumerate}
    \item Left subtrees are full. Each left subtree is a complete binary tree,
        with all its chunks at the same depth, and a number of chunks that is a
        power of 2.
    \item Left subtrees are big. Each left subtree contains a number of chunks
        greater than or equal to the number of chunks in its sibling right
        subtree.
\end{enumerate}
In other words, given a message of $n > 1024$ bytes, the left subtree consists of the first $$2^{10 + \left\lfloor \log_2 \left(\left\lfloor \frac{n-1}{1024} \right\rfloor\right) \right\rfloor}$$ bytes, and the right subtree consists of the remainder.

For example, trees from 1 to 4 chunks have the structure shown in
Figure~\ref{fig:fourchunks}.

\begin{figure}[h]
\centering
\input{img/tree1to4.tikz} 
\caption{Example tree structures, from 1 to 4 chunks.}
\label{fig:fourchunks}
\end{figure}

The compression function is used to derive a chaining value from each chunk and
parent node. The chaining value of the root node, encoded as 32 bytes in
little-endian order, is the default-length BLAKE3 hash of the input. BLAKE3
supports input of any byte length $0 \leq \ell < 2^{64}$.

\alert{Lots of references to ``default length''. Given that the output is not different between different sizes---in theory, BLAKE3's output is an infinite-length string---it's much less of an issue than in BLAKE2.}

\subsection{Compression Function}\label{sec:compression}

The compression function takes an 8-word chaining value, a 16-word message
block, and a 4-word parameter, and it returns a new 16-word value. A word is 32 bits. The inputs to
the compression function are:

\begin{itemize}
    \item The input chaining value, $h_{0} \ldots h_{7}$.
    \item The message block, $m_{0} \ldots m_{15}$.
    \item A 64-bit counter, $t=t_{0},t_{1}$, with $t_{0}$ the lower order word
        and $t_{1}$ the higher order word.
    \item The number of input bytes in the block, $b$.
    \item A set of domain separation bit flags, $d$.
\end{itemize}

The compression function initializes its 16-word internal state $v_{0} \ldots
v_{15}$ as follows: 

\begin{equation*}
\begin{pmatrix}
v_{0} & v_{1} & v_{2} & v_{3} \\
v_{4} & v_{5} & v_{6} & v_{7} \\
v_{8} & v_{9} & v_{10} & v_{11} \\
v_{12} & v_{13} & v_{14} & v_{15} \\
\end{pmatrix}
\leftarrow
\begin{pmatrix}
h_{0} & h_{1} & h_{2} & h_{3} \\
h_{4} & h_{5} & h_{6} & h_{7} \\
\IV_{0} & \IV_{1} & \IV_{2} & \IV_{3} \\
t_{0} & t_{1} & b & d 
\end{pmatrix}
\end{equation*}

The $\IV_{0} \ldots \IV_{7}$ constants are the same as in
BLAKE2s, and they are reproduced in Appendix~\ref{sec:ivconstants}.

The compression function applies a 7-round keyed permutation $v' = E(m, v)$ to
the state $v_0 \dots v_{15}$, keyed by the message $m_0 \dots m_{15}$. \alert{The
keyed permutation here is identical to that of BLAKE2s, and is
reproduced in Appendix~\ref{sec:roundfn}.}

The output of the compression function $h_{0}' \ldots h_{15}'$ is
defined as:
\begin{align*}
h_{0}'  \ & \leftarrow \ v'_{0} \oplus  v'_{8} &
h_{8}'  \ & \leftarrow \ v'_{8} \oplus  h_{0} \\
h_{1}'  \ & \leftarrow \ v'_{1} \oplus  v'_{9} &
h_{9}'  \ & \leftarrow \ v'_{9} \oplus  h_{1} \\
h_{2}'  \ & \leftarrow \ v'_{2} \oplus  v'_{10} &
h_{10}' \ & \leftarrow \ v'_{10} \oplus  h_{2} \\
h_{3}'  \ & \leftarrow \ v'_{3} \oplus  v'_{11} &
h_{11}' \ & \leftarrow \ v'_{11} \oplus  h_{3} \\
h_{4}'  \ & \leftarrow \ v'_{4} \oplus  v'_{12} &
h_{12}' \ & \leftarrow \ v'_{12} \oplus  h_{4} \\
h_{5}'  \ & \leftarrow \ v'_{5} \oplus  v'_{13} &
h_{13}' \ & \leftarrow \ v'_{13} \oplus  h_{5} \\
h_{6}'  \ & \leftarrow \ v'_{6} \oplus  v'_{14} &
h_{14}' \ & \leftarrow \ v'_{14} \oplus  h_{6} \\
h_{7}'  \ & \leftarrow \ v'_{7} \oplus  v'_{15} &
h_{15}' \ & \leftarrow \ v'_{15} \oplus  h_{7}\,.
\end{align*}
If we define $v_l$ (resp. $v'_l$) and $v_h$ (resp. $v'_h$) as the first and last 8-words of the input (resp. output) of $E(m, v)$ as elements of $\mathbb{F}_2^{256}$, the compression function may be represented as the affine transformation
\[
  \begin{pmatrix}
    1 & 1 \\
    0 & 1
  \end{pmatrix}\cdot%
  \begin{pmatrix}
  v'_l \\ v'_h
  \end{pmatrix} + 
  \begin{pmatrix}
   0 \\ v_l
  \end{pmatrix}\,.
\]
The output of the compression function is often truncated to produce 256-bit chaining values. %We defer to analysis of the compression function to \S\ref{sec:security}.

%\subsubsection{Flags}
The compression function input $d$ is a bitfield, with each individual flag consisting of a power of 2. The value of $d$ is the sum of all the flags defined for a given
compression. Their names and values are given in Table~\ref{tab:flags}.
\begin{table}
  \centering
  \caption{Admissible values for input $d$ in the BLAKE3 compression function.}%
  \label{tab:flags}
  \begin{tabular}{cc}
    \toprule
    Flag name             & Value \\ \midrule
    \flag{CHUNK_START} & $2^0$ \\
    \flag{CHUNK_END}   & $2^1$ \\
    \flag{PARENT}       & $2^2$ \\
    \flag{ROOT}         & $2^3$ \\
    \flag{KEYED_HASH}  & $2^4$ \\
    \flag{DERIVE_KEY}  & $2^5$ \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Modes}\label{sec:modes}

BLAKE3 defines three domain-separated modes: \flag{hash},
\flag{keyed_hash}, and \flag{derive_key}. The modes differ from each
other in their key words $k_{0} \ldots k_{7}$ and in the additional flags they
set for every call to the compression function:

\begin{itemize}
  \item \flag{hash}: $k_{0} \ldots k_{7}$ are the constants $\IV_0 \ldots \IV_7$, and no additional flags are set. 
  \item \flag{keyed_hash}: $k_{0} \ldots k_{7}$ are parsed in little-endian order from the 32-byte key given by the caller, and the \flag{KEYED_HASH} flag is set for every compression.
  \item \flag{derive_key}: the key is again given by the caller, and the \flag{DERIVE_KEY} flag is set for every compression.
\end{itemize}

\subsection{Chunk Chaining Values}\label{sec:chunk}

Processing a chunk is structurally similar to the sequential hashing mode of BLAKE2. Each chunk of up to 1024 bytes is split into blocks of up to 64 bytes.
The last block of the last chunk may be shorter, but not empty, unless the
entire input is empty. The last block, if necessary, is padded with zeros to be 64
bytes. 

Each block is parsed in little-endian order into message words $m_{0}
\ldots m_{15}$ and compressed. The input chaining value $h_{0} \ldots h_{7}$
for the first block of each chunk is comprised of the key words $k_{0} \ldots k_{7}$. The
input chaining value for subsequent blocks in each chunk is the output of the truncated
compression function for the previous block. 

The remaining compression function parameters are handled as follows (see also Figure~\ref{fig:chunk} for an example):
\begin{itemize}
\item The counter $t$ for each block is the chunk index, i.e., $0$ for all
blocks in the first chunk, $1$ for all blocks in the second chunk, and so on.
\item The block length $b$ is the
number of input bytes in each block, i.e., $64$ for all full blocks except the last block of
the last chunk, which may be short. 
\item The first block of each chunk sets the
\flag{CHUNK_START} flag (cf. Table~\ref{tab:flags}), and the last block of each chunk sets the
\flag{CHUNK_END} flag. If a chunk contains only one block, that block sets
both \flag{CHUNK_START} and \flag{CHUNK_END}. If a chunk is the root of
its tree, the last block of that chunk also sets the \flag{ROOT} flag.
\end{itemize}

The output of the truncated compression function for the last block in a chunk
is the chaining value of that chunk.

\begin{figure}
\centering
\input{img/chunk.tikz}
\caption{Example of compression function inputs when hashing a 181-byte input $(m_0, m_1, m_2)$ into a 128-byte output $(h_0, h_1)$. Trapezia indicate that the compression function output is truncated to 256 bits.}\label{fig:chunk}
\end{figure}

\subsection{Parent Node Chaining Values}\label{sec:parent}

Each parent node has exactly two children, each either a chunk or another
parent node. The chaining value of each parent node is given by a single call
to the compression function. The input chaining value $h_{0} \ldots h_{7}$ is
the key words $k_{0} \ldots k_{7}$. The message words $m_{0} \ldots m_{7}$ are
the chaining value of the left child, and the message words $m_{8} \ldots
m_{15}$ are the chaining value of the right child. The counter $t$ for parent
nodes is always 0. The number of bytes $b$ for parent nodes is always 64.
Parent nodes set the \flag{PARENT} flag. If a parent node is the root of the
tree, it also sets the \flag{ROOT} flag. The output of the truncated
compression function is the chaining value of the parent node.

\subsection{Extendable Output}\label{sec:extendable}

BLAKE3 can produce outputs of any byte length up to $2^{64}$ bytes. 
This is done by repeating the root compression---that is, the very last 
call to the compression function, which sets the \flag{ROOT} flag---with 
incrementing values of the counter $t$. The results of these repeated root 
compressions are then concatenated to form the output.

When building the output, BLAKE3 uses the full output of the
compression function (cf. \S\ref{sec:compression}). Each 16-word output is
encoded as 64 bytes in little-endian order.

Observe that based on \S\ref{sec:chunk} and \S\ref{sec:parent} above, the first
root compression always uses the counter value $t = 0$. That is either because
it is the last block of the only chunk, which has a chunk index of $0$, or
because it is a parent node. After the first root compression, as long as more
output bytes are needed, $t$ is incremented by 1, and the root compression is
repeated on otherwise the same inputs. If the target output length is not a
multiple of 64, the final compression output is truncated.

Because the repeated root compressions differ only in the value of $t$, the
implementation can execute any number of them in parallel. The caller can also
adjust $t$ to seek to any point in the output stream.

Note that in contrast to BLAKE2 and BLAKE2X, BLAKE3 does not domain separate
outputs of different lengths. Shorter outputs are prefixes of longer ones.

\section{Performance}\label{sec:performance}

\begin{figure}[h]
\centering
%\input{./benchmarks/results/avx2_rayon.pgf}
\input{./img/avx2_rayon.tikz}
\caption{Throughput for a multi-threaded implementation of BLAKE3 at various
    input lengths on an Intel Kaby Lake processor.}
\label{fig:avx2_rayon}
\end{figure}

% TODO: This figure compares an AVX-512 implementation of BLAKE3 to an AVX2
% implementation of BLAKE2, making BLAKE2 look worse than it should. We need a
% proper AVX-512 implementation of BLAKE2, using the new rotation instructions.
\begin{figure}[h]
\centering
%\input{./benchmarks/results/avx512_detect_c.pgf}
\input{./img/avx512_detect_c.tikz}
\caption{Throughput for a single-threaded implementation of BLAKE3 at various
    input lengths on an Intel Skylake-SP processor.}%
\label{fig:avx512_c}
\end{figure}

\begin{figure}[h]
\centering
%\input{./benchmarks/results/rpizero.pgf}
\input{./img/rpizero.tikz}
\caption{Throughput at various input lengths on an ARM1176 processor.}%
\label{fig:rpizero}
\end{figure}

Several factors contribute to the performance of BLAKE3, depending on the
platform and the size of the input:

\begin{itemize}
    \item The tree structure allows an implementation to compress multiple
        chunks in parallel using SIMD (cf.~\S\ref{sec:simd}). With enough
        input, this can occupy SIMD vectors of any width.
    \item The tree structure allows an implementation to use multiple threads.
        With enough input, this can occupy any number of cores.
    \item The compression function uses fewer rounds than in BLAKE2.
    \item The compression function performs well on smaller architectures.
\end{itemize}

Figure~\ref{fig:avx2_rayon} shows the throughput of a multi-threaded
implementation of BLAKE3 on a typical laptop computer. This benchmark ran on an
Intel Kaby Lake i5-8250U processor with 4 physical cores, supporting 256-bit
AVX2 vector instructions, and with Turbo Boost disabled. In this setting, the
implementation remains single-threaded for inputs up to 16~KiB. Parallel
compression using SIMD begins with 8~KiB inputs using 128-bit vectors, and with
16~KiB inputs using 256-bit vectors. The implementation begins multi-threading
with 32~KiB inputs, in this case causing a performance drop. (An implementation
tuned for this specific platform would forgo multi-threading for inputs shorter
than e.g.\ 128~KiB.) Peak throughput occurs at 4~MiB of input, at which point
BLAKE3 is $5\times$ faster than BLAKE2bp and BLAKE2sp and an order of magnitude
faster than BLAKE2b, BLAKE2s, SHA-2, and SHA-3.

Figure~\ref{fig:avx512_c} shows the throughput of a single-threaded
implementation of BLAKE3 on modern server hardware. This benchmark ran on an
AWS c5.metal instance with an Intel Skylake-SP 8175M processor, supporting
AVX-512 vector instructions, and with Turbo Boost disabled. Here, BLAKE3 is the
only algorithm able to take advantage of 512-bit vector arithmetic. (Note that
the KangarooTwelve algorithm, not included in this benchmark, can also use
512-bit vectors.) The fixed tree structures of BLAKE2bp and BLAKE2sp limit
those algorithms to 256-bit vectors.

Figure~\ref{fig:rpizero} shows the throughput of BLAKE3 on a smaller embedded
platform. This benchmark ran on a Raspberry Pi Zero with a 32-bit ARM1176
processor, without multiple cores or SIMD support. Here, BLAKE2b, SHA-512, and
SHA-3 perform relatively poorly, because their compression functions require
64-bit arithmetic. BLAKE3 and BLAKE2s have similar performance profiles here,
and the BLAKE3 compression function uses fewer rounds.

Figures \ref{fig:avx2_rayon}, \ref{fig:avx512_c}, and \ref{fig:rpizero} also
highlight that BLAKE3 performs well for short inputs. The fixed tree structures
of BLAKE2bp and BLAKE2sp are costly when the input is short, because they
always compress a parent node and a fixed number of leaves. The advantage of
their fixed tree structures, however, comes at medium input lengths around
1--4\,KiB, where BLAKE3 does not yet have enough chunks to operate in parallel.
This is the regime where BLAKE2bp and BLAKE2sp pull ahead of BLAKE3 in figures
\ref{fig:avx2_rayon} and \ref{fig:avx512_c}.

\section{Security}\label{sec:security}

TODO

\subsection{Security Goals}\label{sec:goals}

BLAKE3 targets $128$-bit security for all of its security goals. That is, $128$-bit security against (second-)preimage, collision, or differentiability attacks.

The key length is nevertheless $256$ bits, which is useful against multi-target or possible certificational attacks.

Users who need $256$-bit security can remain using BLAKE2.

On why 128-bit security is enough~\cite{TMC}.

\subsection{Mode of Operation}\label{sec:mode}

Indifferentiability / tree soundness stuff here~\cite{DBLP:journals/ijisec/BertoniDPA14,DBLP:journals/tosc/LuykxMN16,DBLP:journals/tosc/DaemenMA18}.

Explain sufficient conditions satisfied..

\subsection{Cryptanalysis}\label{sec:cryptanalysis}

The rather aggressive round reduction from 10 to 7 rounds is based on existent cryptanalysis, along with novel research.

See~\cite{TMC}.

\begin{itemize}
  \item Cryptanalysis of BLAKE~\cite{DBLP:conf/cans/SuWWD10,DBLP:journals/ipl/VidaliNP10,DBLP:journals/iet-ifs/BaiYWW15,DBLP:conf/fse/AumassonGKMM10,DBLP:journals/iacr/JiL09,DK11,DBLP:conf/fse/BiryukovNR11}
  \item Cryptanalysis of BLAKE2~\cite{DBLP:conf/ctrsa/0001KNWW14,DBLP:conf/cisc/Hao14,DBLP:conf/crypto/EspitauFK15}
  \item Table~\ref{tab:trails} shows that the initialization of BLAKE2/3, by restricting inputs, makes things harder for the attacker, with probabilities getting very low very quickly. As also observed in~\cite[\S7]{DBLP:conf/ctrsa/0001KNWW14}. 
  \item The boomerang attacks of, e.g., \cite{DBLP:conf/cisc/Hao14,DBLP:conf/fse/BiryukovNR11,DBLP:journals/iet-ifs/BaiYWW15} exploit the high probabilities on the first few rounds to connect two trails and get through a few more rounds. This is a usual trick to get around the very quick lowering of probabilities in ARX constructs, along with differential-linear attacks. But such attacks do not present a threat to BLAKE3, considering its $128$-bit security target, and input restrictions on the compression function input (i.e., the IV).
\end{itemize}

\begin{table}
\centering
\caption{Best differential trail probabilities for increasing round numbers of the compression functions of BLAKE-256, BLAKE2s, and BLAKE3, respecting their constraints on inputs to the keyed permutation. Probabilities marked with ${}^\ast$ indicate symmetric differences were sought exclusively.}%
\label{tab:trails}
\begin{tabular}{lccccccc}
  \toprule
  Function & 0.5   & 1     & 1.5   & 2     & 2.5   & 3     & 3.5 \\ \midrule
  BLAKE-256 c.f. & $2^{-0}$ & $2^{-0}$ & $2^{-0}$ & $2^{-1}$ & $2^{-6}$ & $2^{-7}$ & $2^{-38}$  \\
  BLAKE\{2s,3\} c.f.   &  $2^{-0}$  & $2^{-0}$ &  $2^{-0}$  & $2^{-1}$ & $2^{-32}$   & $2^{-88} \le p<2^{-48}$ & -  \\
  BLAKE\{-256,2s,3\} & $2^{-0}$ & $2^{-1}$ & $2^{-32}$ & ${\ge 2^{-190}}^\ast$ & - & - & -  \\ % 2^{-190} with 8-symmetric $\Delta$, $2^{-230}$ with 4-symmetric $\Delta$ m10 = 0x88888888
  BLAKE-256 c.f. (CV only) & $2^{-0}$ & $2^{-2}$ & $2^{-12}$ & $2^{-39}$ & - & - & - \\
  BLAKE\{2s,3\} c.f. (CV only) & $2^{-0}$ & $2^{-7}$ & $2^{-32}$ & ${\ge 2^{-161}}^\ast$ & - & - & - \\ % 2^{-161} with 8-symmetric $\Delta$, $2^{-161}$ with 4-symmetric $\Delta$
  \bottomrule
\end{tabular}
\end{table}

\section{Implementation}\label{sec:implementation}

\subsection{Incremental Hashing}\label{sec:incremental}

An incremental implementation of BLAKE3 needs two major components: the state
of the current chunk and a stack of subtree chaining values (the ``CV stack'').
The chunk state is structurally similar to an incremental implementation of the
sequential mode of BLAKE2. Simplifying the management of the CV stack is
especially important for a concise and correct implementation of BLAKE3. This
section is best read together with
\href{https://github.com/veorq/BLAKE3/blob/master/reference_impl/reference_impl.rs}{the code of the reference implementation}.
% TODO: repoint this to BLAKE3-team and a specific tag

\subsubsection{Chunk State}\label{sec:chunkstate}

The chunk state contains the 32-byte CV of the previous block and a 64-byte
input buffer for the next block, and typically also the compression function
parameters $t$ and $d$. Input bytes from the caller are copied into the buffer
until it is full. Then the buffer is compressed together with the previous CV,
using the truncated compression function. The output CV overwrites the previous
CV, and the buffer is cleared. An important detail here is that the last block
of a chunk is compressed differently, setting the \flag{CHUNK_END} flag and
possibly the \flag{ROOT} flag. In an incremental setting, any block could
potentially be the last, until the caller has supplied enough input to place at
least 1 byte in the block after. For that reason, the chunk state waits to
compress the next block until both the buffer is full and the caller has
supplied more input. Note that the CV stack takes the same approach immediately
below: chunk CVs are added to the stack only after the caller supplies at least
1 byte for the following chunk.

\subsubsection{Chaining Value Stack}\label{sec:cvstack}

To help picture the role of the CV stack, Figure~\ref{fig:incrementaltrees}
shows a growing tree as chunk CVs are added incrementally. As just discussed
above, chunk CVs are added only after the caller has supplied at least 1 byte
for the following chunk, so we know that these chunks and parent nodes are not
the root of the tree, and we do not need to worry about the \flag{ROOT} flag
yet. When the first chunk CV is added, $c_0$, it is alone. When the second
chunk CV is added, $c_1$, it completes a subtree of size 2, and we can compute
the first parent CV, $p_0$. The third chunk CV, $c_2$, does not complete any
subtrees. The fourth chunk CV, $c_3$, completes two subtrees, one of size 2 and
one of size 4. First it joins with $c_2$ giving a new parent CV, $p_1$. Then
$p_1$ joins with $p_0$, giving $p_2$.

\begin{figure}[h]
\centering
\input{img/cvstack.tikz}
\caption{An incomplete tree growing incrementally from 1 to 4 chunks. Dotted
    boxes represent CVs that no longer need to be stored.}
\label{fig:incrementaltrees}
\end{figure}

Note that once a subtree is complete, none of its child CVs need to be stored
any longer. We store the CVs that we still need in the CV stack. We can look
through Figure~\ref{fig:incrementaltrees} again, this time focusing on the
solid and dotted boxes, with dotted boxes representing the CVs that no longer
need to be stored. When $c_0$ is alone, it is the only entry in the CV stack.
When $c_1$ is added, $c_0$ is removed from the stack and merged into $p_0$, and
$p_0$ becomes the only entry. When $c_2$ is added, both $p_0$ and $c_2$ remain
in the stack. When $c_3$ is added, $c_2$ and $p_0$ are both removed and merged
into $p_2$, and $p_2$ is the only entry that remains. In this sense, $c_1$,
$c_3$, and $p_1$ are never directly stored in the stack.

Thus the key question for managing the CV stack is, when a new chunk CV is
added, how many subtrees does it complete? Which is to say, how many CVs should
we pop off the stack and merge with it first?

To answer that question, note the following invariant. At each step, the number
of CVs remaining in the stack is the same as the number of 1 bits in the binary
representation of the total number of chunks so far. With one chunk in the tree
(0b001), the stack contains one CV. With two chunks in the tree (0b010), the
stack still contains one CV. With three chunks in the tree (0b011), the stack
now contains two CVs. And with four chunks in the tree (0b100), the stack once
again contains one CV. When we eventually reach seven chunks (0b111), the stack
will contain three CVs.

\alert{To understand why this works, decompose the number of chunks hashed so far in its base 2 form: $b_0\cdot 2^0 + b_1\cdot 2^1 + \dots + b_{63}\cdot 2^{63}$. Each $b_i \ne 0$ corresponds to a full power-of-2 subtree that has been completed and whose CV can be stored without its children. (Saying the same as above, but powers of 2 make it seem less magical than bits.)}

This invariant leads to an algorithm for adding a chunk CV to the stack:
Continue popping CVs off the stack to form new parent nodes, as long as the
number of entries in the stack is greater than or equal to the number of 1 bits
in the new total number of chunks.
Listing~\ref{listing:push_chunk_chaining_value} shows this algorithm as it
appears in the Rust reference implementation.

\begin{listing}[h]
\begin{minted}[fontsize=\footnotesize]{rust}
fn push_chunk_chaining_value(&mut self, mut cv: [u32; 8], total_chunks: u64) {
    // The new chunk chaining value might complete some subtrees along the
    // right edge of the growing tree. For each completed subtree, pop its
    // left child CV off the stack and compress a new parent CV. After as
    // many parent compressions as possible, push the new CV onto the
    // stack. The final length of the stack will be the count of 1 bits in
    // the total number of chunks so far.
    let final_stack_len = total_chunks.count_ones() as u8;
    while self.cv_stack_len >= final_stack_len {
        cv = parent_output(&self.pop_stack(), &cv, &self.key, self.chunk_state.flags)
            .chaining_value();
    }
    self.push_stack(&cv);
}
\end{minted}
\caption{The algorithm in the Rust reference implementation that manages the
    chaining value stack when a new chunk CV is added.}
\label{listing:push_chunk_chaining_value}
\end{listing}

Once the caller indicates that the input is complete, the current chunk is
finalized, and the new chunk CV is merged with each entry in the CV stack from
top to bottom. This happens regardless of the number of chunks so far,
reflecting the fact that subtrees along the right edge of the tree may be
incomplete. The last parent node compression in this case sets the
\flag{ROOT} flag. Or if there are no CVs in the stack, and thus no parent
nodes to compress, finalization of the last (and only) chunk sets the
\flag{ROOT} flag.

\subsection{Multi-threading}\label{sec:multithreading}

Most of the work of computing a BLAKE3 hash is compressing chunks. Each chunk
can be compressed independently, and one approach to multi-threading is to farm
out individual chunks or groups of chunks to tasks on a thread pool. In this
approach, a leader thread owns the chaining value stack
(cf.~\S\ref{sec:cvstack}) and awaits a CV from each task in order.

This leader-workers approach has some inefficiencies. Spawning tasks and
creating channels usually require heap allocation, which is a performance cost
that needs to be amortized over larger groups of chunks. At high degrees of
parallelism, managing the CV stack itself can become a bottleneck.

A more efficient approach to multi-threading is based on a recursive tree
traversal. The input is split into left and right parts. As per the rules in
\S\ref{sec:tree}, the left part receives the largest power-of-2 number of
chunks that leaves at least 1 byte for the right part. Each part then repeats
this splitting step recursively, until the parts are chunk-sized, and each
chunk is compressed into a CV. On the way back up the callstack, each pair of
left and right child CVs is compressed into a parent CV.

This recursive approach fits well into a fork-join concurrency model, like
those provided by OpenMP, Cilk, and Rayon (Rust). Each left and right part
becomes a separate task, and a work-stealing runtime parallelizes those tasks
across however many threads are available. This can work without heap
allocation, because the runtime can make a fixed-size stack allocation at each
recursive callsite.

The recursive approach is simplest when the entire input is available at once,
since no CV stack is needed. In an incremental setting, a hybrid approach is
also possible. A large buffer of input can be compressed recursively into a
single subtree CV, and that CV can be pushed onto the CV stack using the same
algorithm as in \S\ref{sec:cvstack}. If each incremental input is a fixed
power-of-2 number of chunks in size (for example if all input is copied into an
internal buffer before compression), the push algorithm works with no
modification. If each input is a variable size (for example if input from the
caller is compressed directly without copying), the implementation needs to
maintain the largest-to-smallest ordering invariant of the CV stack. The size
of each subtree being compressed must evenly divide the total number of input
bytes received so far, and the implementation might need to break up the
caller's input into separate pieces.

\subsection{SIMD}\label{sec:simd}

There are two approaches to using SIMD in a BLAKE3 implementation, and both are
important for high performance at different input lengths. The first approach
is to use 128-bit vectors to represent the 4-word rows of the state matrix. The
second approach is to use vectors of any size to represent words in multiple
states, which are compressed in parallel.

The first approach is similar to how SIMD is used in BLAKE2b or BLAKE2s, and it
is applicable to inputs of any length, particularly short inputs where the
second approach does not apply. The state $v_0 \ldots v_{15}$ is arranged into
four 128-bit vectors. The first vector contains the state words $v_0 \ldots
v_3$, the second vector contains the state words $v_4 \ldots v_7$, and so on.
Implementing the G function (Appendix~\ref{sec:roundfn}) with vector
instructions thus mixes all four columns of the state matrix in parallel. A
diagonalization step then rotates the words within each row so that each
diagonal now lies along a column, and the vectorized G function is repeated to
mix diagonals. Finally the state is undiagonalized, to prepare for the column
step of the following round.

The second approach is similar to how SIMD is used in BLAKE2bp or BLAKE2sp. In
this approach, multiple chunks are compressed in parallel, and each vector
contains one word from the state matrix of each chunk. That is, the first
vector contains the $v_0$ word from each state, the second vector contains the
$v_1$ word from each state, and so on, using 16 vectors in total. The width of
the vectors determines the number of chunks, so for example 128-bit vectors
compress 4 chunks in parallel, and 256-bit vectors compress 8 chunks in
parallel. Here the G function operates on one column or diagonal at a time, but
across all of the states, and no diagonalization step is required. When enough
input is available, this approach is much more efficient than the first
approach. It also scales to wider instruction sets like AVX2 and AVX-512.

\alert{For SIMD implementations with multiple 128-bit lanes, such as AVX2 and AVX-512, there is a 3rd approach that simply replicates the first approach without transposing the state / message. This might be advantageous in some short-message regimes to be determined.}

The second approach can be integrated with the CV stack algorithm from
\S\ref{sec:cvstack} by computing multiple chunk CVs in parallel and then
pushing each of them into the CV stack one at a time. It can also be combined
with either of the multi-threading strategies from \S\ref{sec:multithreading}.
Rather than having each task or recursive leaf compress one chunk at a time,
each can compress multiple chunks in parallel.

\subsection{Memory Requirements}\label{sec:memory}

BLAKE3 has a larger memory requirement than BLAKE2, because of the chaining
value stack described in \S\ref{sec:cvstack}. An incremental implementation
needs space for a 32-byte chaining value for every level of the tree below the
root. The maximum input size is $2^{64}-1$ bytes, and the chunk size is
$2^{10}$ bytes, giving a maximum tree depth of $64 - 10 = 54$. The CV stack
thus requires $54 \cdot 32 = 1728$ bytes. The chunk state
(cf.~\S\ref{sec:chunkstate}) also requires at least 104 bytes for the chaining
value, the message block, and the chunk counter. The size of the reference
implementation is 1880 bytes on the callstack.

For comparison, BLAKE2s has a memory footprint similar to the BLAKE3 chunk
state alone, at least 104 bytes. BLAKE2b has twice the chaining value size and
block size, requiring at least 200 bytes. And the parallel modes BLAKE2bp and
BLAKE2sp both require at least 776 bytes.

Space-constrained implementations of BLAKE3 can save space by restricting the
maximum input size. For example, the maximum size of an IPv6 ``jumbogram'' is
$2^{32}-1$ bytes, or just under 4~GiB. At this size, the tree depth is 22 and
the CV stack is $22 \cdot 32 = 704$ bytes. For another example, the maximum
size of a TLS record is $2^{14}$ bytes, or exactly 16~KiB. At this size, the
tree depth is 4 and the CV stack is $4 \cdot 32 = 128$ bytes.

\section{Applications}\label{sec:applications}

As a general-purpose hash function, BLAKE3 is suitable whenever a
collision-resistant or preimage-resistant hash function is needed to map
some arbitrary-size input to a fixed-length output.
BLAKE3 further supports keyed modes---in order to be used as a pseudorandom
function, MAC, or key derivation function---as well as streaming and
incremental processing features.

\subsection{Pseudorandom Function and MAC}\label{sec:mac}

Like BLAKE2, BLAKE3 has explicit support for a keyed mode,
\flag{keyed_hash}. This removes the need for a separate construction like
HMAC. The \flag{keyed_hash} mode is also more efficient than keyed BLAKE2 or
HMAC for short messages. BLAKE2 requires an extra compression for the key
block, and HMAC requires three extra compressions. The \flag{keyed_hash}
mode in BLAKE3 does not require any extra compressions.

\subsection{Key Derivation}\label{sec:kdf}

BLAKE3 has explicit support for a key derivation mode, \flag{derive_key}. In
this mode the input bytes should be a hardcoded, globally unique context
string. A good default format for such strings is \texttt{"[application] [date]
[purpose]"}, e.g., \texttt{"example.com 2019-12-25 16:18:03 session tokens
v1"}. If the key material is not already 256 bits, it can be converted to 256
bits using the \flag{hash} mode. The \flag{derive_key} mode is intended to
replace the BLAKE2 personalization parameter for its most common use cases.

Key derivation can encourage better security than personalization, by
cryptographically isolating different components of an application from one
another. This limits the damage that one component can cause by accidentally
leaking its key. When an application needs to use one secret in multiple
algorithms or contexts, the best practice is to derive a separate key for each
use case, such that the original secret is only used with the key derivation
function. However, because the BLAKE3 modes are domain-separated, it is also
possible to use the same key with \flag{keyed_hash} and with
\flag{derive_key}. This can be necessary for backwards compatibility when an
application adds a new use case for an existing key, if the original use case
did not include a context-specific key derivation step.

Like \flag{keyed_hash}, \flag{derive_key} does not require any extra
compressions. For context strings up to 64 bytes and derived key lengths up to
64 bytes, \flag{derive_key} is a single compression. For comparison, HKDF
generally requires eight compressions.

\subsection{Stateful Hash Object}\label{sec:sho}

Trevor's thing: \url{https://github.com/noiseprotocol/sho_spec/blob/master/sho.md}

Due to its zero-overhead keying and variable-length output, BLAKE3 may be adapted into a SHO quite easily. Let $l$ be some initial label. The initial key---the key at each step representing the SHO state---is obtained as $k_0 =\text{\flag{derive_key}}(l)$, after which we have $k_{i+1}, c_{i+1} = \text{\flag{keyed_hash}}(k_i, m_i)$, where $m_i$ is the input at step $i$, and $c_i$ is the ``squeezing'' output.

This SHO should be sound as long as BLAKE3 behaves like a random function, and there are no key collisions. So up to about $2^{128}$ compression function calls.

\subsection{Verified Streaming}\label{sec:verifiedstreaming}

Because BLAKE3 is a tree hash, it supports new use cases that serial hash
functions do not. One new use case is verified streaming. Consider a video
streaming application that fetches video files from an untrusted host. The
application knows the hash of the file it wants, and it needs to verify that
the video data it receives matches that hash. With a serial hash function,
nothing can be verified until the application has downloaded the entire file.
But with BLAKE3, verified streaming is possible. The application can verify and
play individual chunks of video as soon as they arrive.

To verify an individual chunk without re-hashing the entire file, we verify
each parent node on the path from the root to that chunk. For example, suppose
the file is composed of four chunks, like the four-chunk tree in
Figure~\ref{fig:fourchunks}. To verify the first chunk, we start by fetching
the root node. (Specifically, we fetch its message bytes, the concatenated
chaining values of its children.) We compute the root node's CV as per
\S\ref{sec:parent} and confirm that it matches the 32-byte BLAKE3 hash of the
entire file. Then, we fetch the root node's left child, which is the first
chunk's parent. We compute that node's CV and confirm that it matches the first
32-bytes of the root node. Finally, we fetch the first chunk itself. We compute
its CV as per \S\ref{sec:chunk} and verify that it matches the first 32-bytes
of its parent. This verifies that the first chunk is authentic, and we can pass
it along to application code.

To continue streaming, we can immediately fetch the second chunk. It shares the
parent node of the first chunk, and its CV should match the second 32 bytes of
that parent node. For the third chunk, we need to fetch its parent. The CV of
that parent node should match the second 32 bytes of the root node, and then
the CV of the third chunk should match the first 32 bytes of its parent. Note
that whenever we fetch a parent node, we immediately use its first 32 bytes to
check its left child's CV, and then we store its second 32 bytes to check its
right child in the future. We can represent this with a stack of expected CVs.
We push CVs onto this stack and pop them off, as we would with node pointers in
a depth-first traversal of a binary tree. Note that this is different from the
"CV stack" used for incremental hashing in \S\ref{sec:cvstack}; that stack
holds CVs we have computed in the past, while this stack holds CVs we expect to
compute in the future, and we manage them with different algorithms.

Observe that if the application eventually verifies the entire file, it will
have fetched all the nodes of the tree in pre-order. This suggests a simple
wire format for a streaming protocol: The host can concatenate all the parent
nodes and chunks together in pre-order and serve the concatenated bytes as a
single stream. If the client application knows the length of the file in
advance, it does not need any other information to parse the stream and verify
each node. In this format, the space overhead of the parent nodes approaches
two CVs per chunk, or 6.25\%.

If the client does not know the length of the file in advance, it may receive
the length from the host, either separately or at the front of the stream. In
this case, the length is untrusted. If the host reports an incorrect length,
the effect will be that at least the final chunk will fail verification. For
this reason, if an application reads the file sequentially, it does not need to
explicitly verify the length. The stream may terminate with an error partway
through if verification fails, as it might if the network connection failed. An
important edge case is that, if the reported length is 0, the file consists of
a single empty chunk, and the implementation must not forget to verify that the
file hash matches the empty chunk's CV. (One way to make this mistake is for
the implementation to return end-of-file as soon as the current read position
equals the expected length, verifying nothing in the zero-length case.)

On the other hand, if an application implements seeking, length verification is
required. Seeking past the reported end-of-file, or performing an EOF-relative
seek, would mean trusting an unverified length. In these cases, the
implementation must first verify the length by seeking to and verifying the
final chunk. If that succeeds, the length is authentic, and the implementation
can proceed with the caller's requested seek.

This verified streaming protocol has been implemented by the
\href{https://github.com/oconnor663/bao}{Bao} tool. It is conceptually similar
to the existing
\href{https://adc.sourceforge.io/draft-jchapweske-thex-02.html}{Tree Hash
Exchange} format, and to the
\href{https://www.bittorrent.org/beps/bep_0030.html}{BEP~30} extension of the
BitTorrent protocol. However, neither of those prevents length extension, and
the latter (if extracted from the BitTorrent protocol) does not provide
second-preimage resistance~\cite[\S8.5]{DBLP:journals/tosc/DaemenMA18}.

\subsection{Incremental Update}\label{sec:incrementalupdate}

Another new use case supported by BLAKE3 is incrementally updating the root
hash when an input is edited in place. A serial hash function can efficiently
append new bytes to the end of an input, but editing bytes at the beginning or
in the middle requires re-hashing everything to the right of the edit. With
BLAKE3, an application can edit bytes anywhere in its input and re-hash just
the modified chunks and their transitive parent nodes.

For example, consider an input composed of four chunks, again like the
four-chunk tree in Figure~\ref{fig:fourchunks}. Suppose we overwrite some bytes
in the second chunk. To update the root hash, we first compute the second
chunk's new CV as per \S\ref{sec:chunk}. Then we recompute the CV for the
second chunk's parent as per \S\ref{sec:parent}. Note that because the first
chunk is unchanged, the first 32 message bytes for this parent node are
unchanged also, and we do not need to re-read the first chunk. Finally, we
recompute the CV of the root node, and we similarly do not need to re-read
anything from the right half of the tree.

Note that this strategy cannot efficiently insert or remove bytes from the
middle of an input. That would change the position of all the bytes to the
right of the edit and force re-hashing. This is similar to the constraints of a
typical filesystem, where appends and in-place edits are efficent, but
insertions and removals within a file are either slow or unsupported.

\section{Rationales}\label{sec:rationales}

This section goes into more detail about some of the performance tradeoffs and
security considerations in the BLAKE3 design.

\subsection{Chunk Size}\label{sec:chunksize}

BLAKE3 uses 1~KiB chunks. The chunk size is a tradeoff between the peak
throughput for long inputs, which benefits from larger chunks, and the degree
of parallelism for medium-length inputs, which benefits from shorter chunks.

The benefit of a larger chunk size is that the tree contains fewer parent
nodes, and the implementation spends less time compressing them. The number of
parent nodes is equal to the number of chunks minus one, so doubling the chunk
size cuts the number of parent nodes in half. However, note that parent nodes
at the same level of the tree can be compressed in parallel, so having twice as
many parent nodes does not necessarily mean that compressing them takes twice
as much time.

Speaking of which, the benefit of a smaller chunk size is that the
implementation can do more work in parallel. This is irrelevant for very long
inputs, where all the SIMD lanes and possibly CPU cores of the machine will be
occupied regardless. But at medium input lengths, this has a huge impact on
performance. Consider an 8~KiB input. With a chunk size of 1~KiB, this input
can occupy 8 SIMD lanes, which lets the implementation use AVX2 vector
instructions on modern x86 platforms. But if we increased the chunk size to 2
KiB, the same input would only occupy 4 SIMD lanes, and the high throughput of
AVX2 would be left on the table.

Thus we want to pick the smallest chunks size we can to maximize medium-length
parallelism, without incurring "excessive" overhead from parent nodes. The
point of comparison here is the peak throughput for a very large chunk size,
like 64~KiB, where parent node overhead becomes small enough to be negligible.
In our measurements, we find that implementations with 1~KiB chunks can reach
at least 90\% of that peak throughput on both modern x86 platforms and on the
ARM1176.

There are other minor performance considerations to be aware of. A larger chunk
size slightly reduces the space requirement of the CV stack as described in
\S\ref{sec:memory}. Also, verified streaming requires buffering chunks (see
\S\ref{sec:verifiedstreaming}), and incremental updates require re-hashing
chunks (see \S\ref{sec:incrementalupdate}), so both of those use cases benefit
from a small chunk size.

\subsection{Word Size}\label{sec:wordsize}

BLAKE3 uses 32-bit words. The performance tradeoffs in the choice between
32-bit and 64-bit words are complex, and they depend on both the platform and
the size of the input.

Many sources note that SHA-512 (because it uses 64-bit words) is faster on
64-bit platforms than SHA-256 (because it uses 32-bit words). A similar effect
applies to BLAKE2b and BLAKE2s. However, this effect does not apply to
algorithms that incorporate more SIMD parallelism. BLAKE2sp has higher peak
throughput than BLAKE2bp on x86-64. This is because the SIMD instructions that
operate on 8 lanes of 32-bit words have the same throughput as those that
operate on 4 lanes of 64-bit words. The key factor for exploiting SIMD
performance is not word size but vector size, and both of those cases occupy a
256-bit vector. (The remaining effect making BLAKE2sp faster is that its
smaller state requires fewer rounds of compression.)

For that reason, peak throughput on x86-64 is largely independent of the word
size. The performance differences that remain are restricted to shorter input
lengths: 32-bit words perform better for lengths less than one block, while
64-bit words perform better for lengths between one block and one chunk,
comparable to BLAKE2s and BLAKE2b. These effects are minor, and short input
performance is usually dominated by other sources of overhead in typical
applications.

Instead, the decisive advantage of 32-bit words over 64-bit words is
performance on smaller architectures and embedded systems. BLAKE3 aims to be to
be a single general-purpose hash function, suitable for replacing both BLAKE2b
and BLAKE2s. On 32-bit platforms like the ARM1176, where BLAKE2s does
especially well, a 64-bit hash function could be a performance regression.
Choosing 32-bit words is a substantial benefit for these platforms, with little
or no downside for x86-64. Cross-platform flexibility is also important for
protocol designers, who have to consider the performance of their designs
across a wide range of hardware, and who are becoming increasingly skeptical of
designs supporting multiple algorithms.\cite{WG}

\subsection{Compression Function Feed Forward}\label{sec:feedforward}

\subsection{Chunk Counter}\label{sec:chunkcounter}

\subsection{Tree Fanout}\label{sec:treefanout}

\subsection{Domain Flags}\label{sec:domainflags}

% TODO: Let me know what you guys think about having an Acknowledgments section.
\section*{Acknowledgments}\label{sec:acknowledgments}

Thanks to Liming Luo for many discussions about the tree layout.

%\nocite{*}
\bibliographystyle{plainurl}
\bibliography{blake3}

% [These references are placeholders. TODO.]

\begin{appendices}

\section{IV Constants}\label{sec:ivconstants}

    The constants $\IV_0 \ldots \IV_7$ used by the compression function are the
    same as in BLAKE2s. They are:
\begin{align*}
    \IV_0 &= \text{\texttt{0x6a09e667}} &
    \IV_1 &= \text{\texttt{0xbb67ae85}} \\
    \IV_2 &= \text{\texttt{0x3c6ef372}} &
    \IV_3 &= \text{\texttt{0xa54ff53a}} \\
    \IV_4 &= \text{\texttt{0x510e527f}} &
    \IV_5 &= \text{\texttt{0x9b05688c}} \\
    \IV_6 &= \text{\texttt{0x1f83d9ab}} &
    \IV_7 &= \text{\texttt{0x5be0cd19}}
\end{align*}

\section{Round Function}\label{sec:roundfn}

    The compression function transforms the internal state $v_{0} \ldots
    v_{15}$ through a sequence of 7 rounds. The round function is the same as
    in BLAKE2s. A round does:
\begin{align*}
    \GG_{0}&(v_{0}, v_{4}, v_{8}, v_{12}) &
    \GG_{1}&(v_{1}, v_{5}, v_{9}, v_{13}) &
    \GG_{2}&(v_{2}, v_{6}, v_{10}, v_{14}) &
    \GG_{3}&(v_{3}, v_{7}, v_{11}, v_{15}) \\
    \GG_{4}&(v_{0}, v_{5}, v_{10}, v_{15}) &
    \GG_{5}&(v_{1}, v_{6}, v_{11}, v_{12}) &
    \GG_{6}&(v_{2}, v_{7}, v_{8}, v_{13}) &
    \GG_{7}&(v_{3}, v_{4}, v_{9}, v_{14})
\end{align*}

    That is, a round applies a G function to each of the columns in parallel,
    and then to each of the diagonals in parallel. $\GG_i(a, b, c, d)$ is
    defined as follows. $\oplus$~denotes bitwise exclusive-or, $\ggg$~denotes
    bitwise right rotation, and $m_{\sigma_r(x)}$ is the message word whose
    index is the $x$\textsuperscript{th} entry in the message schedule for
    round $r$:
\begin{align*}
    a \ & \leftarrow \ a + b + m_{\sigma_r(2i)} \\
    d \ & \leftarrow \ (d \oplus a) \ggg 16 \\
    c \ & \leftarrow \ c + d \\
    b \ & \leftarrow \ (b \oplus c) \ggg 12 \\
    a \ & \leftarrow \ a + b + m_{\sigma_r(2i+1)} \\
    d \ & \leftarrow \ (d \oplus a) \ggg 8 \\
    c \ & \leftarrow \ c + d \\
    b \ & \leftarrow \ (b \oplus c) \ggg 7
\end{align*}

    The message schedules $\sigma_r$ are:

\begin{center}
\begin{tabular}{ | r | r r r r r r r r r r r r r r r r | }
    \hline
    $\sigma_0$ & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\
    $\sigma_1$ & 14 & 10 & 4 & 8 & 9 & 15 & 13 & 6 & 1 & 12 & 0 & 2 & 11 & 7 & 5 & 3 \\
    $\sigma_2$ & 11 & 8 & 12 & 0 & 5 & 2 & 15 & 13 & 10 & 14 & 3 & 6 & 7 & 1 & 9 & 4 \\
    $\sigma_3$ & 7 & 9 & 3 & 1 & 13 & 12 & 11 & 14 & 2 & 6 & 5 & 10 & 4 & 0 & 15 & 8 \\
    $\sigma_4$ & 9 & 0 & 5 & 7 & 2 & 4 & 10 & 15 & 14 & 1 & 11 & 12 & 6 & 8 & 3 & 13 \\
    $\sigma_5$ & 2 & 12 & 6 & 10 & 0 & 11 & 8 & 3 & 4 & 13 & 7 & 5 & 15 & 14 & 1 & 9 \\
    $\sigma_6$ & 12 & 5 & 1 & 15 & 14 & 13 & 4 & 10 & 0 & 7 & 6 & 3 & 9 & 2 & 8 & 11 \\
    \hline
\end{tabular}
\end{center}

\end{appendices}

\end{document}
